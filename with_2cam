import cv2
import numpy as np
import urllib.request
import pyzbar.pyzbar as pyzbar
import threading
import time
import subprocess
from gtts import gTTS
from face_recognition import face_encodings, face_locations, face_distance
from tflite_runtime.interpreter import Interpreter

# Ensure TensorFlow uses CPU only
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"

# Camera URLs
camera_urls = [
    "http://192.168.88.12/cam-hi.jpg",  # Camera 1 (QR Scanner + Face Detection) [FLIP RIGHT]
    "http://192.168.88.6/cam-hi.jpg"   # Camera 2 (Face Detection Only) [FLIP LEFT]
]

# Load TensorFlow Lite Model
model_path = "keras_model.tflite"
interpreter = Interpreter(model_path=model_path)
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Global Variables
encodeListKnown = []
classNames = []
qr_scanned = False
user_id = None
system_ready = False  # System will start only after QR scan

def speak(text):
    try:
        tts = gTTS(text=text, lang="en")
        tts.save("temp_audio.mp3")
        subprocess.Popen(["mpg321", "temp_audio.mp3"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        time.sleep(2)  # Prevents overlapping audio issues
    except Exception as e:
        print(f"Error with gTTS speech: {e}")

# **QR Code Scanning**
def scan_qr():
    global qr_scanned, user_id, system_ready
    print("Waiting for QR Code scan...")
    speak("Waiting for QR Code scan.")

    prev = ""
    while not qr_scanned:
        try:
            img_resp = urllib.request.urlopen(camera_urls[0])
            imgnp = np.array(bytearray(img_resp.read()), dtype=np.uint8)
            frame = cv2.imdecode(imgnp, -1)

            # Flip the first camera right
            frame = cv2.transpose(frame)
            frame = cv2.flip(frame, 1)

            decodedObjects = pyzbar.decode(frame)
            for obj in decodedObjects:
                pres = obj.data.decode("utf-8")
                if prev != pres:
                    print(f"QR Code Detected: {pres}")
                    speak("QR code detected.")
                    prev = pres
                    user_id = pres  
                    qr_scanned = True
                    break  

            cv2.imshow("Camera 1 - QR & Face Detection", frame)
            if cv2.waitKey(1) & 0xFF == 27:
                break
        except Exception as e:
            print(f"QR Scan Error: {e}")
            speak("QR scan failed. Please try again.")

    cv2.destroyAllWindows()
    system_ready = True  # System now starts face detection

# **Face & Object Detection**
def process_camera_stream(url, flip_direction, window_name):
    global system_ready

    while True:
        if not system_ready:
            time.sleep(1)
            continue

        try:
            img_resp = urllib.request.urlopen(url)
            imgnp = np.array(bytearray(img_resp.read()), dtype=np.uint8)
            img = cv2.imdecode(imgnp, -1)
            img = cv2.resize(img, (640, 480))

            # Apply flip based on camera type
            if flip_direction == "right":
                img = cv2.transpose(img)
                img = cv2.flip(img, 1)
            elif flip_direction == "left":
                img = cv2.flip(img, 1)

            # Face Detection
            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            faces = face_locations(img_rgb, model="hog")
            encodesCurFrame = face_encodings(img_rgb, faces)

            for encodeFace, faceLoc in zip(encodesCurFrame, faces):
                faceDist = face_distance(encodeListKnown, encodeFace)
                matchIndex = np.argmin(faceDist) if faceDist.size > 0 else -1
                name = classNames[matchIndex].upper() if matchIndex != -1 and faceDist[matchIndex] < 0.4 else "UNKNOWN"

                if name != "UNKNOWN":
                    speak(f"{name} detected.")

            # Object Detection
            obj_index, prediction = detect_object(img)
            if obj_index is not None:
                class_name = class_names[obj_index].strip()
                confidence_score = prediction[obj_index]
                if confidence_score >= 0.91:
                    print(f"Object Detected: {class_name} (Confidence: {confidence_score:.2f})")
                    speak(f"{class_name} Detected")

            cv2.imshow(window_name, img)

            if cv2.waitKey(1) & 0xFF == ord("q"):
                break
        except Exception as e:
            print(f"Error processing stream from {url}: {e}")

    cv2.destroyAllWindows()

# **Object Detection**
def detect_object(image):
    try:
        img_resized = cv2.resize(image, (224, 224)) 
        input_data = np.asarray(img_resized, dtype=np.float32).reshape(1, 224, 224, 3) / 127.5 - 1
        interpreter.set_tensor(input_details[0]['index'], input_data)
        interpreter.invoke()
        
        # Copy tensor to avoid segmentation faults
        output_data = np.copy(interpreter.get_tensor(output_details[0]['index']))

        return np.argmax(output_data[0]), output_data[0]

    except Exception as e:
        print(f"Error in object detection: {e}")
        return None, None

# **Main Execution**
if __name__ == "__main__":
    scan_qr()  # First scan QR before starting any other process

    # Start face & object detection only after QR scanning
    threading.Thread(target=process_camera_stream, args=(camera_urls[0], "right", "Camera 1 - QR & Face Detection")).start()
    threading.Thread(target=process_camera_stream, args=(camera_urls[1], "left", "Camera 2 - Face Detection")).start()
