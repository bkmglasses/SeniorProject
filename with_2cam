import cv2
import numpy as np
import urllib.request
import pyzbar.pyzbar as pyzbar
import pandas as pd
from datetime import datetime
from face_recognition import face_encodings, face_locations, face_distance
from tflite_runtime.interpreter import Interpreter
from gtts import gTTS
import firebase_admin
from firebase_admin import credentials, firestore
import base64
import os
import gc
import threading
import subprocess
import RPi.GPIO as GPIO
import time
from pytesseract import image_to_string

# Ensure TensorFlow uses CPU only
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"

# Firebase Initialization
cred = credentials.Certificate("/home/bkm/Downloads/senior-ade3b-firebase-adminsdk-9179f-c81bcee4a6.json")
firebase_admin.initialize_app(cred)
db = firestore.client()

# Camera URLs
camera_urls = [
    "http://192.168.88.6/cam-hi.jpg",  # Main camera (QR Scanner + Face & Object Detection) [FLIP RIGHT]
    "http://192.168.88.12/cam-hi.jpg",  # Secondary camera (Face & Object Detection) [FLIP LEFT]
]

# Paths
model_path = "keras_model.tflite"
labels_path = "labels.txt"
attendance_folder = "attendance"
attendance_file = os.path.join(attendance_folder, "Attendance.csv")

# Ensure folders exist
os.makedirs(attendance_folder, exist_ok=True)
if not os.path.exists(attendance_file):
    pd.DataFrame(columns=["Name", "Time"]).to_csv(attendance_file, index=False)

# Load TFLite model
interpreter = Interpreter(model_path=model_path)
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Load labels
with open(labels_path, "r") as file:
    class_names = file.readlines()

# Global Variables
encodeListKnown = []
classNames = []
qr_scanned = False
user_id = None
system_ready = False  # System will start only after QR scan
last_name = None
last_speech_time = None
speech_delay = 2  # Delay time in seconds
frame_count = 0

# GPIO Setup
button_pin = 26
GPIO.setmode(GPIO.BCM)
GPIO.setup(button_pin, GPIO.IN, pull_up_down=GPIO.PUD_DOWN)

# Function to speak text
def speak(text):
    try:
        tts = gTTS(text=text, lang="en")
        tts.save("temp_audio.mp3")
        subprocess.call(["mpg321", "temp_audio.mp3"])  
    except Exception as e:
        print(f"Error with gTTS speech: {e}")

# **Function to scan QR code (Runs first, before any other function)**
def scan_qr():
    global qr_scanned, user_id, system_ready
    print("Waiting for QR Code scan...")
    speak("Waiting for QR Code scan.")

    prev = ""
    while not qr_scanned:
        try:
            img_resp = urllib.request.urlopen(camera_urls[0])
            imgnp = np.array(bytearray(img_resp.read()), dtype=np.uint8)
            frame = cv2.imdecode(imgnp, -1)

            # Flip to the Right
            frame = cv2.transpose(frame)
            frame = cv2.flip(frame, 1)

            decodedObjects = pyzbar.decode(frame)
            for obj in decodedObjects:
                pres = obj.data.decode("utf-8")
                if prev != pres:
                    print(f"QR Code Detected: {pres}")
                    speak("QR code detected.")
                    prev = pres
                    user_id = pres  
                    qr_scanned = True
                    break  

            cv2.imshow("QR Scan", frame)
            if cv2.waitKey(1) & 0xFF == 27:
                break
        except Exception as e:
            print(f"QR Scan Error: {e}")
            speak("QR scan failed. Please try again.")

    cv2.destroyAllWindows()

    # Load user data only after successful QR scanning
    images, classNames = load_images_from_firestore(user_id)
    global encodeListKnown
    encodeListKnown = findEncodings(images)

    if len(encodeListKnown) == 0:
        print("Error: No face data found! System will not start face recognition.")
        speak("Error, no faces detected. Face recognition will not work.")
    else:
        speak("System is now active.")
        system_ready = True  # Now allow face & object detection

# **Function to process face & object detection**
def process_camera_stream(url, flip_direction):
    global system_ready, last_name, last_speech_time

    while True:
        if not system_ready:
            time.sleep(1)  # Wait until system is ready
            continue

        try:
            img_resp = urllib.request.urlopen(url)
            imgnp = np.array(bytearray(img_resp.read()), dtype=np.uint8)
            img = cv2.imdecode(imgnp, -1)
            img = cv2.resize(img, (640, 480))

            # Apply flip based on camera type
            if flip_direction == "right":
                img = cv2.transpose(img)
                img = cv2.flip(img, 1)
            elif flip_direction == "left":
                img = cv2.flip(img, 1)

            # Face Detection
            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            faces = face_locations(img_rgb, model="hog")
            encodesCurFrame = face_encodings(img_rgb, faces)

            for encodeFace, faceLoc in zip(encodesCurFrame, faces):
                faceDist = face_distance(encodeListKnown, encodeFace)
                matchIndex = np.argmin(faceDist) if faceDist.size > 0 else -1
                name = classNames[matchIndex].upper() if matchIndex != -1 and faceDist[matchIndex] < 0.4 else "UNKNOWN"

                if name != "UNKNOWN":
                    speak(f"{name} detected.")

            # Object Detection
            obj_index, prediction = detect_object(img)
            if obj_index is not None:
                class_name = class_names[obj_index].strip()
                confidence_score = prediction[obj_index]
                if confidence_score >= 0.91:
                    print(f"Object Detected: {class_name} (Confidence: {confidence_score:.2f})")
                    speak(f"{class_name} Detected")

            cv2.imshow(f"Camera {url}", img)

            if cv2.waitKey(1) & 0xFF == ord("q"):
                break
        except Exception as e:
            print(f"Error processing stream from {url}: {e}")

    cv2.destroyAllWindows()

# Function to detect objects
def detect_object(image):
    try:
        img_resized = cv2.resize(image, (224, 224)) 
        input_data = np.asarray(img_resized, dtype=np.float32).reshape(1, 224, 224, 3) / 127.5 - 1
        interpreter.set_tensor(input_details[0]['index'], input_data)
        interpreter.invoke()
        output_data = interpreter.get_tensor(output_details[0]['index'])
        return np.argmax(output_data[0]), output_data[0]
    except Exception as e:
        print(f"Error in object detection: {e}")
        return None, None

# **Main Execution**
if __name__ == "__main__":
    scan_qr()  # **First scan QR before starting any other process**

    # **Start face & object detection only after QR scanning**
    threading.Thread(target=process_camera_stream, args=(camera_urls[0], "right")).start()
    threading.Thread(target=process_camera_stream, args=(camera_urls[1], "left")).start()
